{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2024 GitHub\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# African-American English Vernacular English\n",
    "\n",
    "African American Vernacular English (AAVE), also known as African American English (AAE) or Black English, is a distinctive dialect of English spoken primarily by African Americans. It has its roots in the linguistic legacy of African slaves brought to the United States, blending elements of West African languages with English. AAVE is characterized by unique grammatical, phonological, and syntactic features, such as the use of double negatives, the dropping of consonant clusters, and a specific set of verb tenses. Although often stigmatized and misunderstood, AAVE is a legitimate, rule-governed variety of English with deep cultural and historical significance. It serves as a powerful tool of identity and cultural expression within the African-American community.\n",
    "\n",
    "Corpus of Regional African American Language (CORAAL) is a corpus of African American Language Varieties. The data set provides several hours of audio and transcripts from variety of communities. The UF Data studio has added much of the data set to HuggingFace [CORAAL Data Set](https://huggingface.co/datasets/zsayers/CORAAL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Details\n",
    "### CORAAL:ATL (Atlanta, GA 2017; Version 2020.05)\n",
    "\n",
    "CORAAL:ATL consists of 13 primary speakers across 14 audio files, collected in 2017 and 2018 by Patrick Slay Brooks, a music producer in Atlanta ([www.slayinrecords.com](www.slayinrecords.com)), specifically for CORAAL. Speakers represent a modern friendship network in Atlanta, GA. Atlanta has been  described as a “black mecca\" in the South (Hobson 2010), especially in the context of the so- called reverse Great Migration, the movement of African Americans from Northern and Western cities back to the (urban) South. Brooks has a friendship group that highlights a diversity of experiences in Atlanta. Speakers range from being born and raised in Atlanta, to growing up in  places like New York City, Washington DC, and Los Angeles, CA. As with all sub-components,  see metadata for speaker details.\n",
    "\n",
    "Speakers were interviewed by Brooks for CORAAL to fill a 2 x 2 demographic matrix. In file naming, like with CORAAL:PRV and CORAAL:ROC, the socioeconomic group is listed as “0” (e.g., ATL_se0_ag1_m_01_1) to denote no focus on socioeconomic groups (not to indicate a group lower than 1). We have attempted to capture and include in the metadata broad information about speakers’ demographic backgrounds, such as length of residence and other places lived, but leave questions of interpretation up to end users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORAAL:ATL data\n",
    "\n",
    "The 14 audio files are 44.1 kHz, 16 bit, mono in WAV format, totaling 8.6 hours and 93.5K \n",
    "words. Interviews were recording on a Zoom H5 recorder, with either a lapel microphone or an \n",
    "internal microphone, between 2017 and 2018. Interviews are sociolinguistic styled interviews \n",
    "and conversations on topics such as life in Atlanta, and the interviewee’s neighborhood, \n",
    "schooling, and work history.   \n",
    "\n",
    "Speaker numbers are listed in each cell. \n",
    " \n",
    "| Socio-Economic Group | Gender | Age Group 1 (under 29) | Age Group 2 (30 to 50) |\n",
    "|----------------------|--------|-------------------------|-------------------------|\n",
    "| Group 0             | Female | 3                       | 2                       |\n",
    "|                      | Male   | 5                       | 3                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your directory\n",
    "data_directory = '../data/coraal/audio/wav'\n",
    "\n",
    "# Create a list of file paths\n",
    "file_paths = [os.path.join(data_directory, filename) for filename in os.listdir(data_directory) if filename.endswith('.wav')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paths in file_paths:\n",
    "    print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = file_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize some basic properties of the audio recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "sr, y = wavfile.read(test_path)\n",
    "sr = sr/1000 # Change to kHz\n",
    "\n",
    "# Get duration\n",
    "duration = len(y) / sr\n",
    "\n",
    "# Generate a time array for plotting\n",
    "time = np.linspace(0, duration, len(y))\n",
    "\n",
    "num_samples = len(y)\n",
    "num_channels = 1 if len(y.shape) == 1 else y.shape[1]  # Check if mono or stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Audio Information**: Properties of the audio sample (frequency, samples, duration, channels)\n",
    "1. **Waveform**: Amplitude of the audio signal over time.\n",
    "1. **Spectrogram**: Frequency content of the audio signal over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print audio information\n",
    "print(f\"File: {test_path}\")\n",
    "print(f\"Sample Rate: {sr} kHz\")\n",
    "print(f\"Duration: {duration:.2f} seconds\")\n",
    "print(f\"Number of Samples: {num_samples}\")\n",
    "print(f\"Number of Channels: {num_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(time, y)\n",
    "plt.title(f'Waveform of {test_path}')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 2)\n",
    "plt.specgram(y, Fs=sr, NFFT=1024, noverlap=512, cmap='plasma')\n",
    "plt.title('Spectrogram')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(test_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio File Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Duration Analysis**: Calculate the duration of each audio file to understand the average length of interviews.\n",
    "1. **Sample Rate and Channels**: Check the sample rate and number of channels to ensure consistency across your dataset.\n",
    "1. **Signal Visualization**: Plot waveforms of a few audio samples to visualize variations in speech patterns.\n",
    "1. **Spectrograms**: Generate spectrograms to analyze frequency content and observe characteristics of speech patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_properties(filepath):\n",
    "    # Load the audio file\n",
    "    sr, y = wavfile.read(filepath)\n",
    "    sr = sr/1000 # Change to kHz\n",
    "\n",
    "    # Get duration\n",
    "    duration = (len(y) / sr)\n",
    "\n",
    "    # Generate a time array for plotting\n",
    "    time = np.linspace(0, duration, len(y))\n",
    "\n",
    "    num_samples = len(y)\n",
    "    num_channels = 1 if len(y.shape) == 1 else y.shape[1]  # Check if mono or stereo\n",
    "\n",
    "    audio_prop_dict = {\"Audio Path\": filepath,\n",
    "                       \"Sample Rate (kHz)\": sr,\n",
    "                       \"Duration (s)\": duration,\n",
    "                       \"Number of Samples\": num_samples,\n",
    "                       \"Number of Channels\": num_channels}\n",
    "    return audio_prop_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_properties(file_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_prop_list = []\n",
    "\n",
    "for paths in file_paths:\n",
    "    prop = audio_properties(paths)\n",
    "    audio_prop_list.append(prop)\n",
    "\n",
    "print(audio_prop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(audio_prop_list)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_duration = df['Duration (s)'].min()\n",
    "longest_duration = df['Duration (s)'].max()\n",
    "\n",
    "print(\"Shortest Sample Duration:\", shortest_duration,\n",
    "      \"\\nLongest Sample Duration:\", longest_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_sample_freq = df['Number of Samples'].min()\n",
    "longest_sample_freq = df['Number of Samples'].max()\n",
    "\n",
    "print(\"Shortest Sample Frequency:\", shortest_sample_freq,\n",
    "      \"\\nLongest Sample Frequency:\", longest_sample_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your directory\n",
    "data_directory = '../data/coraal/transcript/text/'\n",
    "\n",
    "# Create a list of file paths\n",
    "file_paths = [os.path.join(data_directory, filename) for filename in os.listdir(data_directory) if filename.endswith('.txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_properties(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\"\\t\", index_col=\"Line\")\n",
    "    df['Audio Path'] = filepath\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paths in file_paths:\n",
    "    display(transcript_properties(paths).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df = pd.read_csv(\"../data/coraal/transcript/text/ATL_se0_ag2_m_02_1.txt\",\n",
    "                            delimiter=\"\\t\", index_col=\"Line\")\n",
    "transcript_df['Audio Path'] = '../data/coraal/transcript/text/ATL_se0_ag2_m_02_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting 10 or 15 second segments of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "\n",
    "def audio_properties(filepath, segment_duration=10):\n",
    "    # Load the audio file\n",
    "    sr, y = wavfile.read(filepath)\n",
    "    sr_khz = sr / 1000  # Change to kHz\n",
    "    segments = []\n",
    "\n",
    "    total_duration = len(y) / sr\n",
    "    num_segments = int(np.ceil(total_duration / segment_duration))\n",
    "\n",
    "    for seg_index in range(num_segments):\n",
    "        start_time = seg_index * segment_duration\n",
    "        end_time = min((seg_index + 1) * segment_duration, total_duration)\n",
    "\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "        segment_y = y[start_sample:end_sample]\n",
    "\n",
    "        # Find overlapping content from the transcript\n",
    "        content_segments = []\n",
    "        for index, row in transcript_df.iterrows():\n",
    "            transcript_start = row['StTime']\n",
    "            transcript_end = row['EnTime']\n",
    "\n",
    "            # Check if there's an overlap\n",
    "            if transcript_end > start_time and transcript_start < end_time:\n",
    "                content_segments.append(row['Content'])\n",
    "\n",
    "        # Combine content for the current segment\n",
    "        combined_content = ' '.join(content_segments)\n",
    "\n",
    "        segments.append({\n",
    "            \"Audio Path\": filepath,\n",
    "            \"Start Time (s)\": start_time,\n",
    "            \"End Time (s)\": end_time,\n",
    "            \"Sample Rate (kHz)\": sr_khz,\n",
    "            \"Number of Samples\": len(segment_y),\n",
    "            \"Number of Channels\": 1 if len(segment_y.shape) == 1 else segment_y.shape[1],\n",
    "            \"Content\": combined_content.strip()  # Join all overlapping content\n",
    "        })\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Usage\n",
    "segment = audio_properties(file_paths[0])\n",
    "segment_df = pd.DataFrame(segment)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(segment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eda anaylsis of aave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coraal', '__MACOSX', 'aave-eda.ipynb']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unzipping and extracting all files from coraal.zip to notebook\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "with zipfile.ZipFile('../data/coraal.zip','r') as z:\n",
    "  z.extractall('./')\n",
    "\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./coraal/audio/wav/ATL_se0_ag2_m_02_1.wav with shape torch.Size([1, 112296237]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag2_f_02_1.wav with shape torch.Size([1, 110205903]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag2_m_01_1.wav with shape torch.Size([1, 107670146]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag2_f_01_1.wav with shape torch.Size([1, 103238100]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag2_m_03_1.wav with shape torch.Size([1, 127426946]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag1_m_05_1.wav with shape torch.Size([1, 103732021]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag1_f_03_1.wav with shape torch.Size([1, 79768080]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag1_m_01_1.wav with shape torch.Size([1, 121561647]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag1_m_03_1.wav with shape torch.Size([1, 121751277]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag1_f_01_1.wav with shape torch.Size([1, 82158296]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag1_m_04_1.wav with shape torch.Size([1, 49877098]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag1_m_04_2.wav with shape torch.Size([1, 44011799]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag1_m_02_1.wav with shape torch.Size([1, 107374677]) and sample rate 44100\n",
      "Loaded ./coraal/audio/wav/ATL_se0_ag1_f_02_1.wav with shape torch.Size([1, 97540379]) and sample rate 44100\n",
      "Loaded ./coraal/urls_coraal.txt with content:\n",
      "http://lingtools.uoregon.edu/coraal/userguide/CORAALUserGuide_current.pdf\n",
      "http://lingtools.uoregon.e...\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag2_f_02_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t0.7526\tHey what's going on?\t2.5113\n",
      "2\tATL_int_01\t2.5113\t...\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag2_m_02_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t0.8713\tCan you tell me your name?\t1.9540\n",
      "2\tATL_se0_ag2_...\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag2_f_01_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t0.7774\tOkay.\t1.3967\n",
      "2\tATL_int_01\t1.3967\t(pause 1.03)\t2....\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag2_m_03_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t0.9221\tOkay.\t1.5357\n",
      "2\tATL_int_01\t1.5357\t(pause 0.33)\t1....\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag2_m_01_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t0.3882\tOkay,\t0.9890\n",
      "2\tATL_int_01\t0.9890\t(pause 1.00)\t1....\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag1_m_05_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t1.0768\tOkay.\t1.6027\n",
      "2\tATL_int_01\t1.6027\t(pause 0.43)\t2....\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag1_m_03_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t0.9274\tUm,\t1.7730\n",
      "2\tATL_int_01\t1.7730\t(pause 0.18)\t1.95...\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag1_f_01_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_se0_ag1_f_01\t0.4436\tThey talking about, don't send him to his ...\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag1_f_03_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t0.2095\t/RD-NAME-1/ can I get your full name please?\t2.2...\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag1_m_01_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_Int_01\t1.1560\tYo, um,\t2.5706\n",
      "2\tATL_Int_01\t2.5706\t(pause 1.03)\t...\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag1_m_04_2.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t0.5098\tOkay. We rolling back again.\t2.0308\n",
      "2\tATL_int_01...\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag1_m_04_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t1.0048\tAlright now I'm gonna go ahead and commence with...\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag1_f_02_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t1.1436\tOkay.\t1.7576\n",
      "2\tATL_int_01\t1.7576\t(pause 0.16)\t1....\n",
      "\n",
      "Loaded ./coraal/transcript/text/ATL_se0_ag1_m_02_1.txt with content:\n",
      "Line\tSpkr\tStTime\tContent\tEnTime\n",
      "1\tATL_int_01\t0.1058\tYeah, I'm here with-\t1.5134\n",
      "2\tATL_int_01\t1.5134\t...\n",
      "\n",
      "Loaded ./coraal/metadata/ATL_metadata_2020.05.txt with content:\n",
      "CORAAL.Sub\tVersion.Created\tVersion.Modified\tCORAAL.Spkr\tCORAAL.File\tAudio.Folder\tTarball\tPrimary.Spk...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the coraal directory\n",
    "coraal_dir = './coraal'\n",
    "\n",
    "# List all files in the coraal directory\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(coraal_dir):\n",
    "    for file in files:\n",
    "        all_files.append(os.path.join(root, file))\n",
    "\n",
    "# Separate audio files and text files\n",
    "audio_files = [f for f in all_files if f.endswith('.wav')]\n",
    "text_files = [f for f in all_files if f.endswith('.txt')]\n",
    "\n",
    "# Load and print some information about the audio files\n",
    "for audio_file in audio_files:\n",
    "    waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    print(f\"Loaded {audio_file} with shape {waveform.shape} and sample rate {sample_rate}\")\n",
    "\n",
    "# Load and print some information about the text files\n",
    "for text_file in text_files:\n",
    "    with open(text_file, 'r') as f:\n",
    "        content = f.read()\n",
    "    print(f\"Loaded {text_file} with content:\\n{content[:100]}...\\n\")  # Print the first 100 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCC shape for ./coraal/audio/wav/ATL_se0_ag2_m_02_1.wav: torch.Size([1, 13, 701849])\n"
     ]
    }
   ],
   "source": [
    "import torchaudio.transforms as transforms\n",
    "\n",
    "# Define the MFCC transform\n",
    "mfcc_transform = transforms.MFCC(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mfcc=13,\n",
    "    melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 23, \"center\": False}\n",
    ")\n",
    "\n",
    "# Extract MFCC features from the first audio file\n",
    "waveform, sample_rate = torchaudio.load(audio_files[0])\n",
    "mfcc = mfcc_transform(waveform)\n",
    "print(f\"MFCC shape for {audio_files[0]}: {mfcc.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-to-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
