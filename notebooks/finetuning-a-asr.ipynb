{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig, WhisperFeatureExtractor\n",
    "from jiwer import wer\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_mapping(transcript_dir, audio_dir):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame mapping transcript files to their corresponding audio files based on filenames.\n",
    "    \n",
    "    Args:\n",
    "        transcript_dir (str): Directory containing transcript text files.\n",
    "        audio_dir (str): Directory containing audio files.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns 'Interview', 'Transcript Path', and 'Audio Path'.\n",
    "    \"\"\"\n",
    "    # Get lists of file paths\n",
    "    transcript_files = [f for f in os.listdir(transcript_dir) if f.endswith('.txt')]\n",
    "    audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "    \n",
    "    # Extract file identifiers from filenames\n",
    "    transcript_ids = {os.path.splitext(f)[0] for f in transcript_files}\n",
    "    audio_ids = {os.path.splitext(f)[0] for f in audio_files}\n",
    "    \n",
    "    # Determine the intersection of transcript and audio IDs\n",
    "    common_ids = transcript_ids.intersection(audio_ids)\n",
    "    \n",
    "    # Create the mapping dictionary\n",
    "    file_mapping = {\n",
    "        'Interview': [],\n",
    "        'Transcript Path': [],\n",
    "        'Audio Path': []\n",
    "    }\n",
    "    \n",
    "    for file_id in common_ids:\n",
    "        transcript_path = os.path.join(transcript_dir, file_id + '.txt')\n",
    "        audio_path = os.path.join(audio_dir, file_id + '.wav')\n",
    "        file_mapping['Interview'].append(file_id)\n",
    "        file_mapping['Transcript Path'].append(transcript_path)\n",
    "        file_mapping['Audio Path'].append(audio_path)\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    return pd.DataFrame(file_mapping)\n",
    "\n",
    "\n",
    "def process_transcripts(paths_df):\n",
    "    \"\"\"\n",
    "    Processes transcript files to extract and format relevant data, and combines them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        paths_df (pd.DataFrame): DataFrame with columns 'Interview', 'Transcript Path', and 'Audio Path'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing concatenated and processed transcript data.\n",
    "    \"\"\"\n",
    "    all_transcript_data = []\n",
    "    \n",
    "    for _, row in paths_df.iterrows():\n",
    "        transcript_path = row['Transcript Path']\n",
    "        \n",
    "        # Load transcript data with appropriate delimiter and columns\n",
    "        transcript_df = pd.read_csv(transcript_path, delimiter=\"\\t\", usecols=['StTime', 'EnTime', 'Content'])\n",
    "        \n",
    "        # Convert times from minutes to milliseconds\n",
    "        transcript_df['StTime'] *= 1000\n",
    "        transcript_df['EnTime'] *= 1000\n",
    "        \n",
    "        # Standardize content to uppercase\n",
    "        transcript_df['Content'] = transcript_df['Content'].str.upper()\n",
    "        \n",
    "        # Add the identifier column\n",
    "        transcript_df['Interview'] = row['Interview']\n",
    "        \n",
    "        # Append to the list\n",
    "        all_transcript_data.append(transcript_df)\n",
    "    \n",
    "    # Combine all transcript data into a single DataFrame\n",
    "    return pd.concat(all_transcript_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, transcript_dir, audio_dir, processor, target_sample_rate=16000, target_length_ms=15000, padding_value=0):\n",
    "        \"\"\"\n",
    "        Audio Dataset with Padding for ASR tasks.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): DataFrame containing the dataset information.\n",
    "        - transcript_dir (str): Directory containing transcript files.\n",
    "        - audio_dir (str): Directory containing audio files.\n",
    "        - processor: Processor for audio and text processing.\n",
    "        - target_sample_rate (int): Target sample rate for audio segments.\n",
    "        - target_length_ms (int): Target length for audio segments in milliseconds.\n",
    "        - padding_value (float): Value to use for padding audio segments.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.transcript_dir = transcript_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.processor = processor\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.target_length_samples = int(target_length_ms * target_sample_rate / 1000)\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        start_tm = row['StTime']\n",
    "        end_tm = row['EnTime']\n",
    "        content = row['Content']\n",
    "        interview = row['Interview']\n",
    "        audio_path = os.path.join(self.audio_dir, interview + '.wav')\n",
    "        \n",
    "        # Extract and process audio segment\n",
    "        audio_segment = self.extract_audio_segment(audio_path, start_tm, end_tm)\n",
    "        audio_tensor = self.processor(audio_segment.squeeze().numpy(), sampling_rate=self.target_sample_rate, return_tensors=\"pt\").input_features\n",
    "        \n",
    "        # Tokenize and pad transcription\n",
    "        encodings = self.processor.tokenizer(\n",
    "            content,\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=448\n",
    "        )\n",
    "        \n",
    "        labels = encodings.input_ids.squeeze()\n",
    "        attention_mask = encodings.attention_mask.squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_features': audio_tensor.squeeze(),\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "    \n",
    "    def extract_audio_segment(self, audio_file_path, start_time_ms, end_time_ms):\n",
    "        \"\"\"\n",
    "        Extracts and pads a segment from an audio file based on start and end times.\n",
    "        \n",
    "        Parameters:\n",
    "        - audio_file_path (str): Path to the audio file.\n",
    "        - start_time_ms (int): Start time in milliseconds.\n",
    "        - end_time_ms (int): End time in milliseconds.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Extracted and padded audio segment.\n",
    "        \"\"\"\n",
    "        waveform, sr = torchaudio.load(audio_file_path)\n",
    "        \n",
    "        # Convert milliseconds to sample indices\n",
    "        start_sample = int(start_time_ms * sr / 1000)\n",
    "        end_sample = int(end_time_ms * sr / 1000)\n",
    "        \n",
    "        # Extract segment and resample if necessary\n",
    "        segment = waveform[:, start_sample:end_sample]\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.target_sample_rate)\n",
    "            segment = resampler(segment)\n",
    "        \n",
    "        # Pad or truncate segment\n",
    "        return self.pad_audio_segment(segment)\n",
    "    \n",
    "    def pad_audio_segment(self, segment):\n",
    "        \"\"\"\n",
    "        Pads or truncates an audio segment to the target length.\n",
    "        \n",
    "        Parameters:\n",
    "        - segment (torch.Tensor): The audio segment to be padded.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Padded or truncated audio segment.\n",
    "        \"\"\"\n",
    "        current_length = segment.size(1)\n",
    "        if current_length < self.target_length_samples:\n",
    "            padding_size = self.target_length_samples - current_length\n",
    "            padding = torch.full((segment.size(0), padding_size), self.padding_value)\n",
    "            padded_segment = torch.cat((segment, padding), dim=1)\n",
    "        else:\n",
    "            padded_segment = segment[:, :self.target_length_samples]\n",
    "        return padded_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StTime</th>\n",
       "      <th>Content</th>\n",
       "      <th>EnTime</th>\n",
       "      <th>Interview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11640</th>\n",
       "      <td>2718918.0</td>\n",
       "      <td>(PAUSE 0.14)</td>\n",
       "      <td>2719059.1</td>\n",
       "      <td>ATL_se0_ag1_m_03_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20990</th>\n",
       "      <td>1489578.4</td>\n",
       "      <td>(PAUSE 1.50)</td>\n",
       "      <td>1491083.3</td>\n",
       "      <td>ATL_se0_ag1_f_03_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11291</th>\n",
       "      <td>2338974.0</td>\n",
       "      <td>(PAUSE 0.10)</td>\n",
       "      <td>2339077.6</td>\n",
       "      <td>ATL_se0_ag1_m_03_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>1837608.8</td>\n",
       "      <td>AND A BIG TOY FROG FOR THE KIDS.</td>\n",
       "      <td>1839870.8</td>\n",
       "      <td>ATL_se0_ag2_m_02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10048</th>\n",
       "      <td>907384.2</td>\n",
       "      <td>JORDAN WOULD WHOOP LEBRON ASS</td>\n",
       "      <td>908894.9</td>\n",
       "      <td>ATL_se0_ag1_m_03_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>339232.3</td>\n",
       "      <td>YOU WOULD KNOW THAT, SINCE I GOT PREGNANT.</td>\n",
       "      <td>340745.1</td>\n",
       "      <td>ATL_se0_ag2_f_02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18839</th>\n",
       "      <td>873271.2</td>\n",
       "      <td>YOU KIND OF GET WISDOM FROM THESE TYPE OF PEOP...</td>\n",
       "      <td>877731.1</td>\n",
       "      <td>ATL_se0_ag2_f_01_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>193721.5</td>\n",
       "      <td>MAKING THAT ADJUSTMENT TO A DIFFERENT AREA?</td>\n",
       "      <td>196303.7</td>\n",
       "      <td>ATL_se0_ag2_f_02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>437000.2</td>\n",
       "      <td>WHAT THAT BOY NAME IS?</td>\n",
       "      <td>438250.0</td>\n",
       "      <td>ATL_se0_ag2_m_03_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8114</th>\n",
       "      <td>1407805.9</td>\n",
       "      <td>[YOU MIGHT DROWN] IN THAT BITCH. [YOU MIGHT GE...</td>\n",
       "      <td>1411648.5</td>\n",
       "      <td>ATL_se0_ag2_m_03_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          StTime                                            Content  \\\n",
       "11640  2718918.0                                       (PAUSE 0.14)   \n",
       "20990  1489578.4                                       (PAUSE 1.50)   \n",
       "11291  2338974.0                                       (PAUSE 0.10)   \n",
       "1659   1837608.8                   AND A BIG TOY FROG FOR THE KIDS.   \n",
       "10048   907384.2                      JORDAN WOULD WHOOP LEBRON ASS   \n",
       "2486    339232.3         YOU WOULD KNOW THAT, SINCE I GOT PREGNANT.   \n",
       "18839   873271.2  YOU KIND OF GET WISDOM FROM THESE TYPE OF PEOP...   \n",
       "2383    193721.5        MAKING THAT ADJUSTMENT TO A DIFFERENT AREA?   \n",
       "7346    437000.2                             WHAT THAT BOY NAME IS?   \n",
       "8114   1407805.9  [YOU MIGHT DROWN] IN THAT BITCH. [YOU MIGHT GE...   \n",
       "\n",
       "          EnTime           Interview  \n",
       "11640  2719059.1  ATL_se0_ag1_m_03_1  \n",
       "20990  1491083.3  ATL_se0_ag1_f_03_1  \n",
       "11291  2339077.6  ATL_se0_ag1_m_03_1  \n",
       "1659   1839870.8  ATL_se0_ag2_m_02_1  \n",
       "10048   908894.9  ATL_se0_ag1_m_03_1  \n",
       "2486    340745.1  ATL_se0_ag2_f_02_1  \n",
       "18839   877731.1  ATL_se0_ag2_f_01_1  \n",
       "2383    196303.7  ATL_se0_ag2_f_02_1  \n",
       "7346    438250.0  ATL_se0_ag2_m_03_1  \n",
       "8114   1411648.5  ATL_se0_ag2_m_03_1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your data\n",
    "transcript_dir = '../data/coraal/transcript/text/'\n",
    "audio_dir = '../data/coraal/audio/wav/'\n",
    "\n",
    "paths_df = create_file_mapping(transcript_dir, audio_dir)\n",
    "combined_transcript_df = process_transcripts(paths_df)\n",
    "display(combined_transcript_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StTime</th>\n",
       "      <th>Content</th>\n",
       "      <th>EnTime</th>\n",
       "      <th>Interview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8541</th>\n",
       "      <td>1233479.3</td>\n",
       "      <td>THAT'S IT?</td>\n",
       "      <td>1234205.0</td>\n",
       "      <td>ATL_se0_ag1_f_03_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3640</th>\n",
       "      <td>2471853.3</td>\n",
       "      <td>I TRY TO DROP MY SHIT. EVEN IF IT JUST ONE TRACK,</td>\n",
       "      <td>2474208.1</td>\n",
       "      <td>ATL_se0_ag2_m_03_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627</th>\n",
       "      <td>1149491.3</td>\n",
       "      <td>THAT'S WHAT- THAT'S WHAT-</td>\n",
       "      <td>1150584.3</td>\n",
       "      <td>ATL_se0_ag1_m_02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>822792.2</td>\n",
       "      <td>WRESTLING?</td>\n",
       "      <td>823547.6</td>\n",
       "      <td>ATL_se0_ag2_f_02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>744923.1</td>\n",
       "      <td>THE LEAST IS LIKE TWO FIFTY.</td>\n",
       "      <td>746560.6</td>\n",
       "      <td>ATL_se0_ag1_f_03_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>563400.2</td>\n",
       "      <td>IT WAS</td>\n",
       "      <td>564145.4</td>\n",
       "      <td>ATL_se0_ag1_m_02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>470958.9</td>\n",
       "      <td>NEEDLESS TO SAY I GOT KICKED OUT OF THE CHARTE...</td>\n",
       "      <td>472974.9</td>\n",
       "      <td>ATL_se0_ag1_f_02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>1444149.2</td>\n",
       "      <td>OF MY OWN AND STUFF LIKE THAT. LIKE</td>\n",
       "      <td>1446193.1</td>\n",
       "      <td>ATL_se0_ag2_f_02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>506861.4</td>\n",
       "      <td>OKAY,</td>\n",
       "      <td>507333.4</td>\n",
       "      <td>ATL_se0_ag2_f_02_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4847</th>\n",
       "      <td>1048828.3</td>\n",
       "      <td>JJ WANTED TO, UM, WHERE ONE OF THEM DRESSES LI...</td>\n",
       "      <td>1051452.1</td>\n",
       "      <td>ATL_se0_ag1_f_01_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         StTime                                            Content     EnTime  \\\n",
       "8541  1233479.3                                         THAT'S IT?  1234205.0   \n",
       "3640  2471853.3  I TRY TO DROP MY SHIT. EVEN IF IT JUST ONE TRACK,  2474208.1   \n",
       "2627  1149491.3                          THAT'S WHAT- THAT'S WHAT-  1150584.3   \n",
       "1275   822792.2                                         WRESTLING?   823547.6   \n",
       "8374   744923.1                       THE LEAST IS LIKE TWO FIFTY.   746560.6   \n",
       "2399   563400.2                                             IT WAS   564145.4   \n",
       "6692   470958.9  NEEDLESS TO SAY I GOT KICKED OUT OF THE CHARTE...   472974.9   \n",
       "1474  1444149.2                OF MY OWN AND STUFF LIKE THAT. LIKE  1446193.1   \n",
       "1170   506861.4                                              OKAY,   507333.4   \n",
       "4847  1048828.3  JJ WANTED TO, UM, WHERE ONE OF THEM DRESSES LI...  1051452.1   \n",
       "\n",
       "               Interview  \n",
       "8541  ATL_se0_ag1_f_03_1  \n",
       "3640  ATL_se0_ag2_m_03_1  \n",
       "2627  ATL_se0_ag1_m_02_1  \n",
       "1275  ATL_se0_ag2_f_02_1  \n",
       "8374  ATL_se0_ag1_f_03_1  \n",
       "2399  ATL_se0_ag1_m_02_1  \n",
       "6692  ATL_se0_ag1_f_02_1  \n",
       "1474  ATL_se0_ag2_f_02_1  \n",
       "1170  ATL_se0_ag2_f_02_1  \n",
       "4847  ATL_se0_ag1_f_01_1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out rows with unwanted characters in the 'Content' column\n",
    "pattern = r'[\\(\\)\\[\\]/<>]'\n",
    "filtered_transcript_df = combined_transcript_df[~combined_transcript_df['Content'].str.contains(pattern)].reset_index(drop=True)\n",
    "display(filtered_transcript_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "# subset_size = int(len(filtered_transcript_df)*.25)\n",
    "subset_size = 150\n",
    "data_subset = filtered_transcript_df.sample(subset_size)\n",
    "print(len(data_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# train_df, test_df = train_test_split(filtered_transcript_df, test_size=0.2, random_state=42)\n",
    "train_df, test_df = train_test_split(data_subset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 120\t Test Dataset: 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Dataset: {len(train_df)}\\t Test Dataset: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "config = WhisperConfig.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = AudioDataset(train_df, transcript_dir, audio_dir, processor)\n",
    "test_dataset = AudioDataset(test_df, transcript_dir, audio_dir, processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, processor, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    true_transcriptions = []\n",
    "    predicted_transcriptions = []\n",
    "    \n",
    "    for batch in tqdm(train_loader, total=len(train_loader), desc='Training'):\n",
    "        inputs = batch['input_features'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_features=inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        outputs = model.generate(input_features=inputs)\n",
    "        transcriptions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "        true_transcriptions.extend(processor.batch_decode(labels, skip_special_tokens=True))\n",
    "        predicted_transcriptions.extend(transcriptions)\n",
    "\n",
    "        predicted_transcriptions = [x.upper() for x in predicted_transcriptions]\n",
    "        wer_score = wer(true_transcriptions, predicted_transcriptions)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    total_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    return total_loss, wer_score\n",
    "\n",
    "# Evaluation function\n",
    "def test(model, test_loader, processor, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    true_transcriptions = []\n",
    "    predicted_transcriptions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # for batch in tqdm(test_loader, total=len(test_loader), desc='Testing'):\n",
    "        for batch in test_loader:\n",
    "            inputs = batch['input_features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_features=inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            outputs = model.generate(input_features=inputs)\n",
    "            transcriptions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            true_transcriptions.extend(processor.batch_decode(labels, skip_special_tokens=True))\n",
    "            predicted_transcriptions.extend(transcriptions)\n",
    "\n",
    "    total_loss = total_loss / len(train_loader)\n",
    "    predicted_transcriptions = [x.upper() for x in predicted_transcriptions]\n",
    "    wer_score = wer(true_transcriptions, predicted_transcriptions)\n",
    "    \n",
    "    return total_loss, wer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:36<00:00, 27.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5724, Word Error Rate (WER): 0.3935\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:27<00:00, 25.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1297, Word Error Rate (WER): 0.3949\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:23<00:00, 25.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1216, Word Error Rate (WER): 0.4147\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:23<00:00, 25.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1208, Word Error Rate (WER): 0.4147\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:20<00:00, 25.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1191, Word Error Rate (WER): 0.4147\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:50<00:00, 28.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1192, Word Error Rate (WER): 0.4133\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:30<00:00, 26.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1196, Word Error Rate (WER): 0.4133\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:47<00:00, 28.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1178, Word Error Rate (WER): 0.4133\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:24<00:00, 25.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1187, Word Error Rate (WER): 0.4133\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:18<00:00, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1209, Word Error Rate (WER): 0.4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "num_epochs = 10\n",
    "train_loss_per_epoch = []\n",
    "train_wer_per_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_wer = train(model, train_loader, processor, optimizer, scheduler, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\\t Training Loss: {train_loss:.4f}, Word Error Rate (WER): {train_wer:.4f}\")\n",
    "    train_loss_per_epoch.append(train_loss)\n",
    "    train_wer_per_epoch.append(train_wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\t Test Loss: 0.0305, Word Error Rate (WER): 0.4022\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_wer = test(model, test_loader, processor, device)\n",
    "print(f\"Running inference...\\t Test Loss: {test_loss:.4f}, Word Error Rate (WER): {test_wer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to ../weights/whisper-weights_2024-07-23_16:16:07\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H:%M:%S') # Format: YYYY-MM-DD_HH:MM:SS\n",
    "weights_filepath = \"../weights/whisper-weights_\"\n",
    "\n",
    "torch.save(model.state_dict(), weights_filepath+timestamp)\n",
    "print(f\"Model weights saved to {weights_filepath+timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-to-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
